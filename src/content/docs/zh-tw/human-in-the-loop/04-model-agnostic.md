---
title: 'äººæ©Ÿå”ä½œï¼šæ¨¡å‹ç„¡é—œå¯¦ä½œ'
description: 'ä½¿ç”¨ LangChain å’Œ OpenRouter å»ºæ§‹é©ç”¨æ–¼ä»»ä½• LLM ä¾›æ‡‰å•†çš„å½ˆæ€§äººæ©Ÿå”ä½œä»£ç†ï¼Œäº†è§£é™åˆ¶å’Œå–æ¨'
---

## æ¦‚è¦½

å»ºæ§‹é©ç”¨æ–¼**ä»»ä½• LLM ä¾›æ‡‰å•†**ï¼ˆClaudeã€GPTã€Geminiã€Llamaã€Mistral ç­‰ï¼‰çš„äººæ©Ÿå”ä½œä»£ç†éœ€è¦ä¸åŒæ–¼ä¾›æ‡‰å•†ç‰¹å®šæ–¹æ³•çš„æ¶æ§‹ã€‚æœ¬æŒ‡å—å±•ç¤ºå¦‚ä½•ä½¿ç”¨ **LangChain + OpenRouter** ä¾†å»ºç«‹å½ˆæ€§ã€å¤šæ¨¡å‹çš„ HITL ä»£ç†ã€‚

```mermaid
graph TB
    subgraph Specific["ä¾›æ‡‰å•†ç‰¹å®š"]
        CC[Claude Code<br/>AskUserQuestion]
        OA[OpenAI<br/>Agents SDK]
    end

    subgraph Agnostic["æ¨¡å‹ç„¡é—œ"]
        LC[LangChain] --> OR[OpenRouter]
        OR --> M1[Claude]
        OR --> M2[GPT-4]
        OR --> M3[Gemini]
        OR --> M4[Llama]
    end

```

## æŒ‘æˆ°

### ç‚ºä»€éº¼ä¾›æ‡‰å•†å·¥å…·ç„¡æ³•é‹ä½œ

**Claude Code çš„ `AskUserQuestion`ï¼š**

- âœ… ç„¡ç¸«é«”é©—
- âŒ åƒ…åœ¨ Claude Code CLI ä¸­é‹ä½œ
- âŒ åƒ…é©ç”¨æ–¼ Claude æ¨¡å‹
- âŒ èˆ‡åŸºç¤è¨­æ–½ç·Šå¯†è€¦åˆ

**OpenAI çš„ Agents SDKï¼š**

- âœ… å…§å»ºæ‰¹å‡†åŠŸèƒ½
- âŒ åƒ…é©ç”¨æ–¼ OpenAI æ¨¡å‹
- âŒ SDK ç‰¹å®š API
- âŒ é™æ–¼å…¶æ¨¡å¼

### æˆ‘å€‘éœ€è¦ä»€éº¼

```mermaid
graph TB
    A[æ¨¡å‹ç„¡é—œéœ€æ±‚] --> B[ä»»ä½• LLM ä¾›æ‡‰å•†]
    A --> C[ä»»ä½• UI å±¤]
    A --> D[è‡ªè¨‚é‚è¼¯]
    A --> E[ç‹€æ…‹ç®¡ç†]

    B --> B1[Claude, GPT, Gemini...]
    C --> C1[CLI, Web, Mobile...]
    D --> D1[é©—è­‰, æ—¥èªŒè¨˜éŒ„...]
    E --> E1[æš«åœèˆ‡æ¢å¾©]

```

## æ¶æ§‹é™åˆ¶

### é™åˆ¶ 1ï¼šå·¥å…·å‘¼å«è®Šç•°æ€§

**ä¸¦éæ‰€æœ‰æ¨¡å‹éƒ½å…·æœ‰ç›¸åŒçš„å·¥å…·å‘¼å«èƒ½åŠ›ï¼š**

| ä¾›æ‡‰å•†                 | åŸç”Ÿæ”¯æ´  | å¯é æ€§ | æ³¨æ„äº‹é …       |
| ---------------------- | --------- | ------ | -------------- |
| **Claude** (Anthropic) | âœ… å„ªç§€   | 95%+   | æœ€ä½³çš„å·¥å…·å‘¼å« |
| **GPT-4** (OpenAI)     | âœ… å„ªç§€   | 95%+   | éå¸¸å¯é        |
| **Gemini** (Google)    | âœ… è‰¯å¥½   | 85%+   | é€šå¸¸å¯é        |
| **Llama 3**            | âš ï¸ ä¾æƒ…æ³ | 60-80% | ä¾è³´å¾®èª¿       |
| **Mistral**            | âš ï¸ æœ‰é™   | 50-70% | ç¶“å¸¸éœ€è¦æç¤º   |
| **æœ¬åœ°æ¨¡å‹**           | âŒ è®ŠåŒ–   | 30-60% | é«˜åº¦è®ŠåŒ–       |

**å½±éŸ¿ï¼š** æ‚¨éœ€è¦é‡å°è¼ƒå¼±æ¨¡å‹çš„å‚™æ´ç­–ç•¥ã€‚

### é™åˆ¶ 2ï¼šç„¡æ¨™æº–ã€Œè©¢å•ä½¿ç”¨è€…ã€å·¥å…·

èˆ‡æª”æ¡ˆæ“ä½œæˆ–ç¶²è·¯æœå°‹ä¸åŒï¼Œ**æ²’æœ‰ã€Œæš«åœä¸¦è©¢å•ä½¿ç”¨è€…ã€çš„é€šç”¨æ¨™æº–**ã€‚

é€™æ„å‘³è‘—ï¼š

- âŒ æ‚¨å¿…é ˆè‡ªå·±å®šç¾©å·¥å…·
- âŒ æ‚¨å¿…é ˆè™•ç†åŸ·è¡Œè¿´åœˆä¸­æ–·
- âŒ æ‚¨å¿…é ˆç®¡ç†æš«åœå’Œæ¢å¾©ä¹‹é–“çš„ç‹€æ…‹
- âŒ æ‚¨å¿…é ˆå¯¦ä½œ UI å±¤

### é™åˆ¶ 3ï¼šç‹€æ…‹ç®¡ç†

ç•¶æ‚¨æš«åœè©¢å•ä½¿ç”¨è€…æ™‚ï¼Œå¿…é ˆä¿ç•™ä¸Šä¸‹æ–‡ï¼š

```python
# Current conversation state when pausing
messages = [
    SystemMessage("You are an assistant"),
    HumanMessage("Build a feature"),
    AIMessage("I need to know which database"),
    AIMessage(tool_calls=[{
        "name": "ask_user_question",
        "args": {...}
    }])  # â† Paused here
]

# After user answers:
messages.append(ToolMessage(
    content="PostgreSQL",
    tool_call_id="..."
))  # â† Add answer

# Resume with full context
response = llm.invoke(messages)
```

**å½±éŸ¿ï¼š** æ‚¨çš„æ‡‰ç”¨ç¨‹å¼ç®¡ç†æš«åœä¹‹é–“çš„å°è©±ç‹€æ…‹ã€‚

### é™åˆ¶ 4ï¼šUI æ˜¯æ‡‰ç”¨ç¨‹å¼ç‰¹å®šçš„

åƒ Claude Code é€™æ¨£çš„ä¾›æ‡‰å•†å·¥å…·æœ‰å…§å»º UIã€‚**æ‚¨å¿…é ˆæä¾›è‡ªå·±çš„ï¼š**

- çµ‚ç«¯æ©Ÿï¼ˆCLIã€rich CLIï¼‰
- ç¶²é ï¼ˆReactã€Vueã€Streamlitï¼‰
- è¡Œå‹•è£ç½®ï¼ˆReact Nativeã€Flutterï¼‰
- APIï¼ˆRESTã€GraphQLï¼‰

**å½±éŸ¿ï¼š** ä»£ç†é‚è¼¯èˆ‡ UI æ¸²æŸ“ä¹‹é–“çš„é—œæ³¨é»åˆ†é›¢ã€‚

## LangChain + OpenRouter æ¶æ§‹

### ç‚ºä»€éº¼é¸æ“‡é€™å€‹æŠ€è¡“æ£§ï¼Ÿ

**LangChainï¼š**

- ğŸ”„ æ‰€æœ‰ä¾›æ‡‰å•†çš„çµ±ä¸€ä»‹é¢
- ğŸ› ï¸ æ¨™æº–åŒ–å·¥å…·å‘¼å«
- ğŸ“ è¨Šæ¯æ ¼å¼è½‰æ›
- ğŸ¯ è±å¯Œçš„ç”Ÿæ…‹ç³»çµ±ï¼ˆè¨˜æ†¶ã€å›èª¿ã€éˆï¼‰

**OpenRouterï¼š**

- ğŸŒ 100+ æ¨¡å‹çš„å–®ä¸€ API
- ğŸ”€ è‡ªå‹•è·¯ç”±å’Œå‚™æ´
- ğŸ’° æˆæœ¬å„ªåŒ–
- ğŸ”‘ æ‰€æœ‰ä¾›æ‡‰å•†çš„å–®ä¸€ API é‡‘é‘°

### æ¶æ§‹åœ–

```mermaid
graph TB
    subgraph App["æ‚¨çš„æ‡‰ç”¨ç¨‹å¼"]
        Agent[ä»£ç†æ§åˆ¶å™¨]
        State[ç‹€æ…‹ç®¡ç†å™¨]
        UI[UI è™•ç†å™¨å·¥å» ]
    end

    subgraph LangChain["LangChain å±¤"]
        Chat[ChatModel]
        Tools[å·¥å…·ç¶å®š]
        Msgs[è¨Šæ¯æ¨™æº–åŒ–]
    end

    subgraph OpenRouter["OpenRouter å±¤"]
        Router[æ¨¡å‹è·¯ç”±å™¨]
        Fallback[å‚™æ´é‚è¼¯]
    end

    subgraph Models["LLM ä¾›æ‡‰å•†"]
        Claude[Claude]
        GPT[GPT-4]
        Gemini[Gemini]
        Llama[Llama]
    end

    subgraph UILayer["UI å±¤ï¼ˆå¯æ’æ‹”ï¼‰"]
        CLI[CLI è™•ç†å™¨]
        Web[Web è™•ç†å™¨]
        Mobile[Mobile è™•ç†å™¨]
    end

    Agent --> Chat
    Agent --> State
    Agent --> UI
    Chat --> Tools
    Tools --> Msgs
    Msgs --> Router
    Router --> Fallback
    Fallback --> Claude
    Fallback --> GPT
    Fallback --> Gemini
    Fallback --> Llama
    UI --> CLI
    UI --> Web
    UI --> Mobile

```

## å¯¦ä½œ

### è¨­å®š

```bash
pip install langchain langchain-openai langchain-core
```

```python
import os

# OpenRouter API key (single key for all models)
os.environ["OPENROUTER_API_KEY"] = "your-key"
```

### æ­¥é©Ÿ 1ï¼šå®šç¾©å·¥å…·

```python
from langchain_core.tools import tool
from typing import List

@tool
def ask_user_question(
    question: str,
    options: List[str],
    multi_select: bool = False
) -> str:
    """
    Ask the user a question with multiple choice options.

    Use this when you need user input to make decisions.

    Args:
        question: The question to ask (be clear and specific)
        options: List of 2-4 options for the user to choose from
        multi_select: If True, user can select multiple options

    Returns:
        The user's selected option(s)

    Example:
        ask_user_question(
            question="Which database should we use?",
            options=["PostgreSQL", "MongoDB", "Redis"],
            multi_select=False
        )
    """
    # This is intercepted by the agent's execution loop
    return "PAUSE_FOR_USER_INPUT"
```

### æ­¥é©Ÿ 2ï¼šå»ºç«‹ UI è™•ç†å™¨ä»‹é¢

```python
from abc import ABC, abstractmethod

class UIHandler(ABC):
    """Abstract base for UI implementations"""

    @abstractmethod
    def ask_question(
        self,
        question: str,
        options: List[str],
        multi_select: bool = False
    ) -> str:
        """Render question and get user response"""
        pass


class CLIHandler(UIHandler):
    """Simple command-line interface"""

    def ask_question(
        self,
        question: str,
        options: List[str],
        multi_select: bool = False
    ) -> str:
        print(f"\n{'='*70}")
        print(f"â“ {question}")
        print(f"{'='*70}")

        for i, option in enumerate(options, 1):
            print(f"  {i}. {option}")

        if multi_select:
            print("\nSelect multiple (comma-separated, e.g., 1,3)")

        while True:
            try:
                choice = input(f"\nSelect (1-{len(options)}): ").strip()

                if multi_select:
                    indices = [int(c.strip()) - 1 for c in choice.split(',')]
                    if all(0 <= idx < len(options) for idx in indices):
                        selected = [options[idx] for idx in indices]
                        return ", ".join(selected)
                else:
                    idx = int(choice) - 1
                    if 0 <= idx < len(options):
                        return options[idx]

                print("âŒ Invalid choice. Try again.")
            except (ValueError, KeyboardInterrupt):
                print("âŒ Invalid input.")
```

### æ­¥é©Ÿ 3ï¼šå»ºæ§‹äº’å‹•ä»£ç†

```python
from langchain_core.messages import (
    HumanMessage,
    AIMessage,
    SystemMessage,
    ToolMessage
)
from langchain_openai import ChatOpenAI

class InteractiveAgent:
    """
    Model-agnostic agent with human-in-the-loop support.

    Works with any LLM via OpenRouter + LangChain.
    """

    SYSTEM_PROMPT = """You are a helpful assistant that can interact with users.

When you need user input to make decisions, use the ask_user_question tool.

Guidelines:
- Ask clear, specific questions
- Provide 2-4 well-described options
- Use multi_select=True for non-exclusive choices
- Use multi_select=False for mutually exclusive choices"""

    def __init__(
        self,
        llm,
        tools: List,
        ui_handler: UIHandler = None
    ):
        self.llm = llm
        self.tools = tools
        self.tool_map = {tool.name: tool for tool in tools}
        self.llm_with_tools = llm.bind_tools(tools)
        self.ui_handler = ui_handler or CLIHandler()

    def run(
        self,
        user_request: str,
        max_iterations: int = 20,
        verbose: bool = True
    ) -> str:
        """Run the agent with human-in-the-loop support"""

        messages = [
            SystemMessage(content=self.SYSTEM_PROMPT),
            HumanMessage(content=user_request)
        ]

        if verbose:
            print(f"\n{'='*70}")
            print("ğŸ¤– INTERACTIVE AGENT STARTED")
            print(f"{'='*70}")

        for iteration in range(1, max_iterations + 1):
            if verbose:
                print(f"\n{'â”€'*70}")
                print(f"Iteration {iteration}/{max_iterations}")

            # Call LLM
            response = self.llm_with_tools.invoke(messages)
            messages.append(response)

            # Check if done (no tool calls)
            if not response.tool_calls:
                if verbose:
                    print("\nâœ… COMPLETED")
                return response.content

            # Process tool calls
            for tool_call in response.tool_calls:
                tool_name = tool_call["name"]
                tool_args = tool_call["args"]

                if verbose:
                    print(f"\nâš¡ Tool: {tool_name}")

                # Special handling for ask_user_question
                if tool_name == "ask_user_question":
                    result = self._handle_user_question(tool_args, verbose)
                else:
                    result = self._execute_tool(tool_name, tool_args)

                # Add tool result to messages
                messages.append(ToolMessage(
                    content=str(result),
                    tool_call_id=tool_call["id"]
                ))

        return "âš ï¸ Max iterations reached"

    def _handle_user_question(self, tool_args, verbose):
        """Handle ask_user_question tool call"""
        question = tool_args.get("question", "Please choose")
        options = tool_args.get("options", [])
        multi_select = tool_args.get("multi_select", False)

        if verbose:
            print(f"   Question: {question}")
            print(f"   Options: {len(options)}")

        # Use UI handler to get user input
        answer = self.ui_handler.ask_question(
            question=question,
            options=options,
            multi_select=multi_select
        )

        return answer

    def _execute_tool(self, tool_name, tool_args):
        """Execute a regular tool"""
        if tool_name in self.tool_map:
            tool = self.tool_map[tool_name]
            try:
                return tool.invoke(tool_args)
            except Exception as e:
                return f"Error: {str(e)}"
        else:
            return f"Error: Unknown tool {tool_name}"
```

### æ­¥é©Ÿ 4ï¼šé€é OpenRouter å–å¾— LLM

```python
def get_llm(
    model: str = "anthropic/claude-sonnet-4",
    temperature: float = 0,
    max_tokens: int = 4000
) -> ChatOpenAI:
    """
    Get LLM via OpenRouter.

    Supported models:
    - anthropic/claude-sonnet-4
    - anthropic/claude-opus-4
    - openai/gpt-4-turbo
    - openai/gpt-3.5-turbo
    - google/gemini-pro-1.5
    - meta-llama/llama-3-70b-instruct
    - mistralai/mistral-large
    """
    return ChatOpenAI(
        model=model,
        openai_api_key=os.environ["OPENROUTER_API_KEY"],
        openai_api_base="https://openrouter.ai/api/v1",
        temperature=temperature,
        max_tokens=max_tokens
    )
```

### æ­¥é©Ÿ 5ï¼šä½¿ç”¨æ–¹å¼

```python
# Choose any model via OpenRouter
llm = get_llm("anthropic/claude-sonnet-4")
# Or: llm = get_llm("openai/gpt-4-turbo")
# Or: llm = get_llm("google/gemini-pro-1.5")

# Create agent
tools = [ask_user_question]
agent = InteractiveAgent(llm, tools)

# Run
result = agent.run("Help me set up authentication for my app")
print(result)
```

## é€²éšï¼šè±å¯Œçš„ UI è™•ç†å™¨

### ä½¿ç”¨ InquirerPy çš„çµ‚ç«¯æ©Ÿ UI

```bash
pip install InquirerPy
```

```python
from InquirerPy import inquirer
from InquirerPy.base.control import Choice

class RichCLIHandler(UIHandler):
    """Rich terminal UI with InquirerPy"""

    def ask_question(
        self,
        question: str,
        options: List[str],
        multi_select: bool = False
    ) -> str:
        choices = [Choice(value=opt, name=opt) for opt in options]

        if multi_select:
            selected = inquirer.checkbox(
                message=question,
                choices=choices,
                instruction="(Space to select, Enter to confirm)"
            ).execute()
            return ", ".join(selected)
        else:
            selected = inquirer.select(
                message=question,
                choices=choices,
                instruction="(Use arrow keys, Enter to select)"
            ).execute()
            return selected


# Usage
ui = RichCLIHandler()
agent = InteractiveAgent(llm, tools, ui_handler=ui)
```

### ä½¿ç”¨ Streamlit çš„ç¶²é  UI

```python
import streamlit as st
from queue import Queue
import threading

class StreamlitUIHandler(UIHandler):
    """Web-based UI using Streamlit"""

    def __init__(self):
        self.response_queue = Queue()

    def ask_question(
        self,
        question: str,
        options: List[str],
        multi_select: bool = False
    ) -> str:
        # Store question in session state
        st.session_state.current_question = {
            "question": question,
            "options": options,
            "multi_select": multi_select
        }

        # Wait for user response
        return self.response_queue.get()


# Streamlit app
def main():
    st.title("Interactive AI Agent")

    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Display chat history
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.write(msg["content"])

    # Pending question?
    if "current_question" in st.session_state:
        q = st.session_state.current_question

        st.subheader(q["question"])

        if q["multi_select"]:
            selected = st.multiselect("Choose options:", q["options"])
        else:
            selected = st.radio("Choose one:", q["options"])

        if st.button("Submit"):
            ui_handler.response_queue.put(selected)
            del st.session_state.current_question
            st.rerun()

    # User input
    if prompt := st.chat_input("What would you like to do?"):
        run_agent_in_background(prompt)
```

## æ¯”è¼ƒï¼šæ‰€æœ‰æ–¹æ³•

```mermaid
graph TB
    subgraph CC["Claude Code"]
        CC1[é›¶è¨­å®š<br/>~0 è¡Œç¨‹å¼ç¢¼]
        CC2[è‡ªå‹• UI]
        CC3[åƒ… Claude]
    end

    subgraph OA["OpenAI SDK"]
        OA1[ä¸­ç­‰è¨­å®š<br/>~50 è¡Œç¨‹å¼ç¢¼]
        OA2[åŠè‡ªå‹•<br/>æ‰¹å‡†]
        OA3[åƒ… OpenAI]
    end

    subgraph MA["æ¨¡å‹ç„¡é—œ"]
        MA1[é«˜è¨­å®š<br/>~200+ è¡Œç¨‹å¼ç¢¼]
        MA2[è‡ªè¨‚ UI]
        MA3[ä»»ä½•æ¨¡å‹]
    end

```

| é¢å‘       | Claude Code | OpenAI SDK | æ¨¡å‹ç„¡é—œ |
| ---------- | ----------- | ---------- | -------- |
| **è¨­å®š**   | é›¶          | ä¸­ç­‰       | é«˜       |
| **ç¨‹å¼ç¢¼** | ~0 è¡Œ       | ~50 è¡Œ     | ~200+ è¡Œ |
| **UI**     | è‡ªå‹•ï¼ˆCLIï¼‰ | æ‰‹å‹•       | å®Œå…¨è‡ªè¨‚ |
| **æ¨¡å‹**   | åƒ… Claude   | åƒ… OpenAI  | ä»»ä½•æ¨¡å‹ |
| **éˆæ´»æ€§** | ä½          | ä¸­ç­‰       | é«˜       |
| **è¤‡é›œåº¦** | éå¸¸ä½      | ä¸­ç­‰       | é«˜       |
| **å¯æ”œæ€§** | ç„¡          | ä½         | é«˜       |
| **ç”Ÿç”¢**   | ç¤ºç¯„        | è‰¯å¥½       | æœ€ä½³     |

## å–æ¨

### æ›´å¤šç¨‹å¼ç¢¼ vs æ›´å¤šéˆæ´»æ€§

**Claude Codeï¼š**

```markdown
Use AskUserQuestion to ask which database.
```

**æ¨¡å‹ç„¡é—œï¼š**

```python
# ~200 lines:
# 1. Define tool
# 2. Create UI handler
# 3. Build agent with execution loop
# 4. Handle state management
# 5. Implement error handling
```

### æ‰‹å‹•ç‹€æ…‹ vs å®Œå…¨æ§åˆ¶

**è‡ªå‹•ï¼ˆClaude Codeï¼‰ï¼š**

- âœ… ä¸éœ€è¦ç‹€æ…‹ç®¡ç†
- âŒ ç„¡æ³•è‡ªè¨‚æµç¨‹
- âŒ ç¶å®šæ–¼ä¸€å€‹ä¾›æ‡‰å•†

**æ‰‹å‹•ï¼ˆæ¨¡å‹ç„¡é—œï¼‰ï¼š**

- âŒ å¿…é ˆç®¡ç†å°è©±ç‹€æ…‹
- âœ… å®Œå…¨æ§åˆ¶æµç¨‹
- âœ… é©ç”¨æ–¼ä»»ä½•ä¾›æ‡‰å•†

### é—œæ³¨é»åˆ†é›¢

```mermaid
graph TB
    subgraph Integrated["æ•´åˆï¼ˆClaude Codeï¼‰"]
        I1[å·¥å…· + UI + ç‹€æ…‹<br/>å…¨éƒ¨ä¸€èµ·è™•ç†]
    end

    subgraph Separated["åˆ†é›¢ï¼ˆæ¨¡å‹ç„¡é—œï¼‰"]
        S1[LLM å±¤]
        S2[æ‡‰ç”¨ç¨‹å¼å±¤]
        S3[UI å±¤]
        S1 --> S2 --> S3
    end

```

## ä½•æ™‚ä½¿ç”¨æ¨¡å‹ç„¡é—œ

| âœ… ä½¿ç”¨æ™‚æ©Ÿ...          | âŒ é¿å…ä½¿ç”¨æ™‚æ©Ÿ... |
| ----------------------- | ------------------ |
| éœ€è¦å¤šæ¨¡å‹æ”¯æ´          | Claude Code å·²è¶³å¤  |
| å»ºæ§‹ç”Ÿç”¢ç´šç¶²é /è¡Œå‹•æ‡‰ç”¨ | ç°¡å–® CLI ç¤ºç¯„      |
| éœ€è¦è‡ªè¨‚ UI             | æ¨™æº– UI å³å¯       |
| è¤‡é›œé©—è­‰é‚è¼¯            | ç›´æ¥çš„å·¥ä½œæµç¨‹     |
| è·¨ä¾›æ‡‰å•†è¦æ¨¡åŒ–          | å–®ä¸€ä¾›æ‡‰å•†å³å¯     |

## æœ€ä½³å¯¦è¸

### 1. æ¨¡å‹å‚™æ´ç­–ç•¥

```python
def get_llm_with_fallback(preferred_model: str):
    """Try preferred, fallback to reliable models"""

    models = [
        preferred_model,
        "anthropic/claude-sonnet-4",  # Reliable fallback
        "openai/gpt-4-turbo",          # Another option
    ]

    for model in models:
        try:
            llm = get_llm(model)
            # Test with simple call
            llm.invoke([HumanMessage(content="Hi")])
            return llm
        except Exception as e:
            print(f"Failed {model}: {e}")
            continue

    raise RuntimeError("No working LLM available")
```

### 2. UI å·¥å» æ¨¡å¼

```python
class UIFactory:
    """Create UI handlers based on environment"""

    @staticmethod
    def create(ui_type: str) -> UIHandler:
        if ui_type == "cli":
            return CLIHandler()
        elif ui_type == "rich-cli":
            return RichCLIHandler()
        elif ui_type == "web":
            return StreamlitUIHandler()
        elif ui_type == "api":
            return APIUIHandler()
        else:
            raise ValueError(f"Unknown UI type: {ui_type}")


# Usage
ui_type = os.environ.get("UI_TYPE", "cli")
ui = UIFactory.create(ui_type)
agent = InteractiveAgent(llm, tools, ui_handler=ui)
```

### 3. æ—¥èªŒè¨˜éŒ„èˆ‡ç›£æ§

```python
import json
from datetime import datetime

class LoggingAgent(InteractiveAgent):
    """Agent that logs all interactions"""

    def __init__(self, *args, log_file: str = "interactions.jsonl", **kwargs):
        super().__init__(*args, **kwargs)
        self.log_file = log_file

    def _handle_user_question(self, tool_args, verbose):
        # Log question
        self._log_event("question_asked", tool_args)

        # Get answer
        answer = super()._handle_user_question(tool_args, verbose)

        # Log answer
        self._log_event("question_answered", {"answer": answer})

        return answer

    def _log_event(self, event_type, data):
        with open(self.log_file, 'a') as f:
            log_entry = {
                "timestamp": datetime.now().isoformat(),
                "event": event_type,
                "data": data
            }
            f.write(json.dumps(log_entry) + "\n")
```

### 4. éŒ¯èª¤æ¢å¾©

```python
def run(self, user_request: str, max_iterations: int = 20):
    """Run with error recovery"""
    try:
        messages = [...]

        for iteration in range(max_iterations):
            try:
                response = self.llm_with_tools.invoke(messages)
                # ...

                if tool_name == "ask_user_question":
                    try:
                        answer = self.ui_handler.ask_question(...)
                    except KeyboardInterrupt:
                        return "User cancelled"
                    except Exception as e:
                        # Fallback to simple input
                        answer = input("Error in UI. Enter answer: ")

            except Exception as e:
                print(f"Error in iteration {iteration}: {e}")
                continue

    except Exception as e:
        return f"Agent failed: {str(e)}"
```

## å®Œæ•´ç¯„ä¾‹

ä¸»è¦åŠŸèƒ½ï¼š

- âœ… é€é OpenRouter é©ç”¨æ–¼ä»»ä½•æ¨¡å‹
- âœ… å¯æ’æ‹” UI è™•ç†å™¨ï¼ˆCLIã€Rich CLIã€Webï¼‰
- âœ… ç‹€æ…‹ç®¡ç†
- âœ… éŒ¯èª¤è™•ç†
- âœ… æ—¥èªŒè¨˜éŒ„æ”¯æ´
- âœ… ~200 è¡Œä¹¾æ·¨çš„ç¨‹å¼ç¢¼

## ç¸½çµ

### é—œéµè¦é»

1. **ç„¡é€šç”¨æ¨™æº–**ï¼šæ¯å€‹ä¾›æ‡‰å•†å¯¦ä½œ HITL çš„æ–¹å¼ä¸åŒ
2. **LangChain + OpenRouter**ï¼šæ¨¡å‹ç„¡é—œå¯¦ä½œçš„æœ€ä½³æŠ€è¡“æ£§
3. **æ›´è¤‡é›œã€æ›´å¯æ§**ï¼šç”¨ç¨‹å¼ç¢¼æ›å–éˆæ´»æ€§
4. **é—œæ³¨é»åˆ†é›¢**ï¼šLLMã€æ‡‰ç”¨ç¨‹å¼ã€UI å±¤
5. **ç”Ÿç”¢å°±ç·’**ï¼šé©ç”¨æ–¼ç¶²é /è¡Œå‹•æ‡‰ç”¨

### æ±ºç­–çŸ©é™£

```mermaid
flowchart TD
    A[éœ€è¦ HITLï¼Ÿ] -->|æ˜¯| B{å–®ä¸€ä¾›æ‡‰å•†å¯ä»¥å—ï¼Ÿ}
    B -->|Claude| C[Claude Code<br/>AskUserQuestion]
    B -->|OpenAI| D[OpenAI<br/>Agents SDK]
    B -->|å¦| E{éœ€è¦è‡ªè¨‚ UIï¼Ÿ}
    E -->|æ˜¯| F[æ¨¡å‹ç„¡é—œ<br/>å®Œå…¨è‡ªè¨‚]
    E -->|å¦| G{å“ªå€‹ä¾›æ‡‰å•†ï¼Ÿ}
    G --> C
    G --> D

```

### ä½•æ™‚é¸æ“‡å„æ–¹æ³•

| æƒ…å¢ƒ                      | å»ºè­°              |
| ------------------------- | ----------------- |
| **å»ºæ§‹ Claude Code æŠ€èƒ½** | Claude Code å…§å»º  |
| **åƒ… OpenAI çš„ç”Ÿç”¢æ‡‰ç”¨**  | OpenAI Agents SDK |
| **å¤šæ¨¡å‹ç ”ç©¶å¹³å°**        | æ¨¡å‹ç„¡é—œ          |
| **ç¶²é /è¡Œå‹•æ‡‰ç”¨**         | æ¨¡å‹ç„¡é—œ          |
| **å¿«é€ŸåŸå‹**              | Claude Code å…§å»º  |
| **è·¨ä¾›æ‡‰å•†è¦æ¨¡åŒ–**        | æ¨¡å‹ç„¡é—œ          |

## ä¸‹ä¸€æ­¥

- **å­¸ç¿’åŸºç¤** â†’ å¯©æŸ¥[æ¦‚è¦½](/ai-agent-study/zh-tw/human-in-the-loop/01-overview/)
- **æ¢ç´¢æ¨¡å¼** â†’ æŸ¥çœ‹æœ¬ç³»åˆ—çš„å…¶ä»–æ•™å­¸

## å»¶ä¼¸é–±è®€

- [LangChain Documentation](https://python.langchain.com/docs/)
- [OpenRouter API](https://openrouter.ai/)
- [InquirerPy](https://github.com/kazhala/InquirerPy)
- [Streamlit](https://streamlit.io/)
- [Human-in-the-Loop Design Patterns](https://www.promptingguide.ai/agents/function-calling)
